# llama3_text_skill_grpo.yaml
# --------------------------------------------------------------
# Hyper-parameters for fine-tuning Meta-Llama-3.1-70B on the
# CRMArena-Pro B2B *text-skill* tasks with GRPO + LoRA PEFT.
# This file will be consumed by train_text_skill.py via
#   GRPOConfig.from_yaml("configs/llama3_text_skill_grpo.yaml")
# --------------------------------------------------------------

# ---- model / tokenizer -----------------------------------------------------
model_name: meta-llama/Meta-Llama-3.1-70B
trust_remote_code: true

# ---- training batch sizes --------------------------------------------------
# Updated for 4×H100 (GPUs 0-1 for training, TP=2)
per_device_train_batch_size: 2        # 2 seq / GPU → 4 seq total
gradient_accumulation_steps: 4        # effective 8 seq per step
num_train_epochs: 2
max_steps: -1                         # use epochs

# Allow long system prompt + tool catalogue
max_prompt_length: 4096

# ---- logging & evaluation --------------------------------------------------
logging_steps: 10
save_steps: 250
evaluation_strategy: steps
eval_steps: 250                       # run val split every 250 steps (~¼ epoch)

# ---- optimisation ----------------------------------------------------------
learning_rate: 5.0e-5
lr_scheduler_type: cosine
warmup_ratio: 0.05
adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1.0e-8
max_grad_norm: 1.0

# ---- LoRA settings ---------------------------------------------------------
peft:
  lora_r: 64
  lora_alpha: 128
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj

# ---- Reward normalisation --------------------------------------------------
# enables moving average baseline & scaling inside GRPOTrainer
normalize_rewards: true

# ---- DeepSpeed ZeRO-3 ------------------------------------------------------
deepspeed_config:
  stage: 3
  offload_optimizer_device: cpu
  offload_param_device: none
  reduce_scatter: true
  overlap_comm: true
  allgather_bucket_size: 400000000
  reduce_bucket_size:   400000000

# ---- async generation ------------------------------------------------------
num_batches_ahead: 1                  # keep one batch in flight via vLLM

# ---- W&B -------------------------------------------------------------------
report_to: wandb
wandb_project: crmarena_text_skill
wandb_run_name: llama3_70b_b2b_grpo 